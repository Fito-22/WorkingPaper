{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we have to prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing necessary packages and loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>subtopic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2 2 0 2  g u A 6 2  ] E H . h p - o r t s a [ ...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Draft version August 29, 2022 Typeset using LA...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Astronomy &amp; Astrophysics manuscript no. 41891c...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Astronomy &amp; Astrophysics manuscript no. aa Aug...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2 2 0 2  g u A 6 2  ]  R S . h p - o r t s a [...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                  0      subtopic\n",
       "0           0  2 2 0 2  g u A 6 2  ] E H . h p - o r t s a [ ...  Astrophysics\n",
       "1           1  Draft version August 29, 2022 Typeset using LA...  Astrophysics\n",
       "2           2  Astronomy & Astrophysics manuscript no. 41891c...  Astrophysics\n",
       "3           3  Astronomy & Astrophysics manuscript no. aa Aug...  Astrophysics\n",
       "4           4  2 2 0 2  g u A 6 2  ]  R S . h p - o r t s a [...  Astrophysics"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../raw_data/small_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_count = data.duplicated().sum()\n",
    "duplicate_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for missing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0.0\n",
       "0             0.0\n",
       "subtopic      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sort_values(ascending=False)/len(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rename columns and remove useless columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "data = data.rename(columns={'0':'pdf_content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_content</th>\n",
       "      <th>subtopic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 2 0 2  g u A 6 2  ] E H . h p - o r t s a [ ...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Draft version August 29, 2022 Typeset using LA...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astronomy &amp; Astrophysics manuscript no. 41891c...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Astronomy &amp; Astrophysics manuscript no. aa Aug...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 2 0 2  g u A 6 2  ]  R S . h p - o r t s a [...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>2 2 0 2  g u A 3 2  ]  A F . h t a m  [  1 v 5...</td>\n",
       "      <td>Symplectic Geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>UNIVERSIDAD COMPLUTENSE DE MADRID  FACULTAD DE...</td>\n",
       "      <td>Symplectic Geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>2 2 0 2  g u A 0 2  ]  G S . h t a m  [  1 v 4...</td>\n",
       "      <td>Symplectic Geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>2 2 0 2  g u A 6 1  ]  G S . h t a m  [  1 v 4...</td>\n",
       "      <td>Symplectic Geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>Free ﬁeld realisation and the chiral universal...</td>\n",
       "      <td>Symplectic Geometry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2035 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pdf_content             subtopic\n",
       "0     2 2 0 2  g u A 6 2  ] E H . h p - o r t s a [ ...         Astrophysics\n",
       "1     Draft version August 29, 2022 Typeset using LA...         Astrophysics\n",
       "2     Astronomy & Astrophysics manuscript no. 41891c...         Astrophysics\n",
       "3     Astronomy & Astrophysics manuscript no. aa Aug...         Astrophysics\n",
       "4     2 2 0 2  g u A 6 2  ]  R S . h p - o r t s a [...         Astrophysics\n",
       "...                                                 ...                  ...\n",
       "2030  2 2 0 2  g u A 3 2  ]  A F . h t a m  [  1 v 5...  Symplectic Geometry\n",
       "2031  UNIVERSIDAD COMPLUTENSE DE MADRID  FACULTAD DE...  Symplectic Geometry\n",
       "2032  2 2 0 2  g u A 0 2  ]  G S . h t a m  [  1 v 4...  Symplectic Geometry\n",
       "2033  2 2 0 2  g u A 6 1  ]  G S . h t a m  [  1 v 4...  Symplectic Geometry\n",
       "2034  Free ﬁeld realisation and the chiral universal...  Symplectic Geometry\n",
       "\n",
       "[2035 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing whitespace from the start and the end of each string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data['pdf_content'][i] = data['pdf_content'][i].strip()\n",
    "# data['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turning every letter into lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data['pdf_content'][i] = data['pdf_content'][i].lower()\n",
    "# data['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing digits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data['pdf_content'][i] = ''.join(char for char in data['pdf_content'][i] if not char.isdigit())\n",
    "# data['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    for punctuation in string.punctuation:\n",
    "        data['pdf_content'][i] = data['pdf_content'][i].replace(punctuation, '')\n",
    "# data['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing single letter words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data['pdf_content'][i] = ' '.join(w for w in data['pdf_content'][i].split() if len(w)>1)\n",
    "# data['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing all of the text before abstract, because that is where all of the PDFs start**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data['pdf_content'][i] = re.sub(r\"^.+?(?=abstract)\", \"\", data['pdf_content'][i])\n",
    "    data['pdf_content'][i] = data['pdf_content'][i].lstrip('abstract')\n",
    "    data['pdf_content'][i] = data['pdf_content'][i].lstrip()\n",
    "# data['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This regular expression technically removes every character that is not lowercase alphabet(a-z), but in this case it is used to remove mathematical characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data['pdf_content'][i] = re.sub(r'[^a-z]', ' ',data['pdf_content'][i])\n",
    "# data['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing multiple whitespace with one whitespace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data['pdf_content'][i] = ' '.join(data['pdf_content'][i].split())\n",
    "# data['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_content</th>\n",
       "      <th>subtopic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we present numerical simulation results for th...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forecasting solar energetic particles seps and...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context the spacebased multiband astronomical ...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context in cold and shielded environments mole...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we report the discovery of new example of the ...</td>\n",
       "      <td>Astrophysics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>finite frames or spanning sets for nitedimensi...</td>\n",
       "      <td>Symplectic Geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>due to the emergence of symplectic geometry th...</td>\n",
       "      <td>Symplectic Geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>in this article we modify the classical floer ...</td>\n",
       "      <td>Symplectic Geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>we prove one deformation theoretic extension o...</td>\n",
       "      <td>Symplectic Geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>in the tqft formalism of moore tachikawa for d...</td>\n",
       "      <td>Symplectic Geometry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2035 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pdf_content             subtopic\n",
       "0     we present numerical simulation results for th...         Astrophysics\n",
       "1     forecasting solar energetic particles seps and...         Astrophysics\n",
       "2     context the spacebased multiband astronomical ...         Astrophysics\n",
       "3     context in cold and shielded environments mole...         Astrophysics\n",
       "4     we report the discovery of new example of the ...         Astrophysics\n",
       "...                                                 ...                  ...\n",
       "2030  finite frames or spanning sets for nitedimensi...  Symplectic Geometry\n",
       "2031  due to the emergence of symplectic geometry th...  Symplectic Geometry\n",
       "2032  in this article we modify the classical floer ...  Symplectic Geometry\n",
       "2033  we prove one deformation theoretic extension o...  Symplectic Geometry\n",
       "2034  in the tqft formalism of moore tachikawa for d...  Symplectic Geometry\n",
       "\n",
       "[2035 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Astrophysics', 'Condensed Matter',\n",
       "       'General Relativity and Quantum Cosmology',\n",
       "       'High Energy Physics - Experiment',\n",
       "       'High Energy Physics - Lattice',\n",
       "       'High Energy Physics - Phenomenology',\n",
       "       'High Energy Physics - Theory', 'Mathematical Physics',\n",
       "       'Nonlinear Sciences', 'Nuclear Experiment', 'Nuclear Theory',\n",
       "       'Quantum Physics', 'Algebraic Geometry', 'Algebraic Topology',\n",
       "       'Analysis of PDEs', 'Category Theory',\n",
       "       'Classical Analysis and ODEs', 'Combinatorics',\n",
       "       'Commutative Algebra', 'Complex Variables',\n",
       "       'Differential Geometry', 'Dynamical Systems',\n",
       "       'Functional Analysis', 'General Mathematics', 'General Topology',\n",
       "       'Geometric Topology', 'Group Theory', 'History and Overview',\n",
       "       'Information Theory', 'K-Theory and Homology', 'Logic',\n",
       "       'Metric Geometry', 'Number Theory', 'Numerical Analysis',\n",
       "       'Operator Algebras', 'Optimization and Control', 'Probability',\n",
       "       'Quantum Algebra', 'Representation Theory', 'Rings and Algebras',\n",
       "       'Spectral Theory', 'Statistics Theory', 'Symplectic Geometry'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subtopic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_subtopics = ['Astrophysics', \n",
    "                     'Condensed Matter', \n",
    "                     'General Relativity and Quantum Cosmology',\n",
    "                     'High Energy Physics - Experiment',\n",
    "                     'High Energy Physics - Lattice',\n",
    "                     'High Energy Physics - Phenomenology',\n",
    "                     'High Energy Physics - Theory',\n",
    "                     'Mathematical Physics',\n",
    "                     'Nonlinear Sciences',\n",
    "                     'Nuclear Experiment',\n",
    "                     'Nuclear Theory',\n",
    "                     'Physics',\n",
    "                     'Quantum Physics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topic'] = ''\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['subtopic'][i] in physics_subtopics:\n",
    "        data['topic'][i] = 'Physics'\n",
    "    else:\n",
    "        data['topic'][i] = 'Mathematics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing the pdf content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = data.copy()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    tokenized_df['pdf_content'][i] = word_tokenize(tokenized_df['pdf_content'][i])\n",
    "# tokenized_df['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    tokenized_df['pdf_content'][i] = [w for w in tokenized_df['pdf_content'][i] if w not in stop_words]\n",
    "tokenized_df['pdf_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatizing the tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_df = tokenized_df.copy()\n",
    "\n",
    "for i in range(len(tokenized_df)):\n",
    "    lemmatized_df['pdf_content'][i] = [WordNetLemmatizer().lemmatize(word, pos = \"v\")  # v --> verbs\n",
    "              for word in lemmatized_df['pdf_content'][i]]\n",
    "for i in range(len(tokenized_df)):\n",
    "    lemmatized_df['pdf_content'][i] = [WordNetLemmatizer().lemmatize(word, pos = \"n\")  # n --> nouns\n",
    "              for word in lemmatized_df['pdf_content'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_df['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_lemmatized_df = lemmatized_df.copy()\n",
    "\n",
    "for i in range(len(lemmatized_df)):\n",
    "    joined_lemmatized_df['pdf_content'][i] = ' '.join(word for word in joined_lemmatized_df['pdf_content'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_lemmatized_df['pdf_content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(joined_lemmatized_df['pdf_content'])\n",
    "sequences = tk.texts_to_sequences(joined_lemmatized_df['pdf_content'])\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2035, 45744) 182837\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "X_pad = pad_sequences(sequences, dtype='float32', padding='post')\n",
    "print(X_pad.shape, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding the categorical column topic into binary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "y = data[['topic']]\n",
    "enc = OrdinalEncoder()\n",
    "\n",
    "y = enc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 50)          9141900   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 10)                2440      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,144,351\n",
      "Trainable params: 9,144,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Let's build the neural network now\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1,\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer :)\n",
    "))\n",
    "\n",
    "model.add(layers.LSTM(10))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pad, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  1/128 [..............................] - ETA: 4:14:14 - loss: 0.6927 - accuracy: 0.6250"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('WorkingPaper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42a11f523a6e94b6f72da2d66f2716a5a1ab87d136bd1c0555521e948b5b1345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
